<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://tpeat.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tpeat.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-09T05:04:35+00:00</updated><id>https://tpeat.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Multimodal Foundation Models</title><link href="https://tpeat.github.io/blog/2024/mllm/" rel="alternate" type="text/html" title="Multimodal Foundation Models"/><published>2024-12-05T15:09:00+00:00</published><updated>2024-12-05T15:09:00+00:00</updated><id>https://tpeat.github.io/blog/2024/mllm</id><content type="html" xml:base="https://tpeat.github.io/blog/2024/mllm/"><![CDATA[<h1 id="my-favorites">My Favorites</h1> <h3 id="chameleon-mixed-modal-early-fusion-foundation-models">Chameleon: Mixed-Modal Early-Fusion Foundation Models</h3> <p>The Chamelon work by Meta was one of my favorite reads in a while; I appreciate the focus on truly unifying modalities off the rip <a class="citation" href="#chameleonteam2024chameleonmixedmodalearlyfusionfoundation">(Team, 2024)</a>. It supports my vision that someday all modalities can just be plopped into a model and treated the same; basically abstract the model from knowing what modality it actually is. I think vision modalities still pose a problem to this because they are way higher dimension than text so they would unfairely hog cmopute unless we discrete them with a code book.</p> <p>The model’s architecture is particularly innovative for three main reasons: First, it uses a single transformer architecture that can handle both text and image data through a unified token system. This is analogous to speaking a single universal language rather than constantly translating between different languages. The model was trained on an impressive scale - approximately 10 trillion interleaved tokens - which helped it develop this unified understanding.</p> <p>Second, the researchers identified and solved a critical challenge in multimodal training: competition between different types of input. They discovered that different modalities would essentially “fight” for prominence during training, which they could observe through mathematical analysis of token norms. They developed solutions like QK normalization to prevent this competition, leading to more stable training.</p> <p>Third, the model demonstrates remarkable versatility. It can not only handle mixed image-text tasks as well as specialized models like Gemini and GPT-4V, but it also maintains strong performance on text-only tasks, outperforming LLaMA2 in most evaluations.</p> <p>I think it would be an interesting approach to weave the “thought tokens” from O1 models into the input stream and generate a mix of modality, thought, different modality tokens at the output. In integrate thoughts closer, to what we are actually producing, maybe we can induce better reasoning capabiltiies.</p> <h3 id="imagebind-one-embedding-space-to-bind-them-all">ImageBind: One Embedding Space To Bind Them All</h3> <p>Meta’s ImageBind creates a unified embedding space for six different modalities using only image-paired data as the connective tissue <a class="citation" href="#girdhar2023imagebindembeddingspacebind">(Girdhar et al., 2023)</a>. This innovative approach allows the model to bridge images, text, audio, depth, thermal, and IMU data in ways that enable remarkable zero-shot and few-shot learning capabilities across modalities with limited or no direct pairing data. By keeping pretrained image and text encoders frozen while fine-tuning encoders for other modalities, IMAGEBIND has demonstrated surprising “emergent alignment” properties, where modalities that seemingly shouldn’t be related show natural connections. While built upon CLIP’s architecture (and thus inheriting some of its limitations), IMAGEBIND has already surpassed supervised models like AudioMAE in zero-shot performance.</p> <p>I was quite suprised that images were the binding modality to unify the other 5. I actuall thought that language would be a better representation for the other modalities, as humans invented language to describe the physical world. I think passing modalities like EEG through a vision encoder makes no sense because we are assuming that the frozen vision modal has the capacity to represent the incoming modality as an image. This feels like a bottleneck to modalities that shouldn’t have an image representation.</p> <h3 id="learning-transferable-visual-models-from-natural-language-supervision">Learning Transferable Visual Models From Natural Language Supervision</h3> <p>An all time classic and what feels like a universal solution to open-vocabulary models: CLIP <a class="citation" href="#radford2021learningtransferablevisualmodels">(Radford et al., 2021)</a>. The CLIP model uses natural language supervision, (image, text) pairs scraped from the web to pre-train multiple models and classifiers to perform on a variety of benchmarks without optimizing specifically for a single benchmark. The main contribution of the paper was scaling the concepts of natural language for vision datasets and consequently created a powerful family of zero-shot models that are competitive with fully supervised models and robust to underlying distribution shifts.</p> <p>A really well written paper all around. The scaling analysis was thorough, evaluation on a billion metrics.</p> <p>Limitations: obviously there are concerns of data leakage between tasks.</p> <p>I think CLIPs inability to perform fine grained tasks is pretty interesting and begs the question at whether the captions just aren’t detailed enough, too noisy, or if the problem lies in the contrastive loss objective which might be optimizing for semantic concepts rather than background less-important concepts.</p> <h1 id="at-bat">At Bat</h1> <h3 id="agentinstruct-toward-generative-teaching-with-agentic-flows">AgentInstruct: Toward Generative Teaching with Agentic Flows</h3> <p>I believe the next big leap in AGI might come from training with videos and/or interactions with environments, the latter having the advantage that models can create their own data through the interactions <a class="citation" href="#mitra2024agentinstructgenerativeteachingagentic">(Mitra et al., 2024)</a>.</p> <p>There’s some research suggesting that scaling might be plateuring and thus companies like Anthropic or OpenAI are turning to agents; see OpenAI’s Operator.</p> <h3 id="vipact-visual-perception-enhancement-via-specialized-vlm-agent-collaboration-and-tool-use">VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use</h3> <p><a class="citation" href="#zhang2024vipactvisualperceptionenhancementspecialized">(Zhang et al., 2024)</a></p> <p>Reasoning tasks.</p> <h3 id="hugginggpt-solving-ai-tasks-with-chatgpt-and-its-friends-in-hugging-face">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</h3> <p><a class="citation" href="#shen2023hugginggptsolvingaitasks">(Shen et al., 2023)</a></p> <p>Reasoning tasks.</p> <h3 id="m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought">\(M^3\)CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought</h3> <p><a class="citation" href="#chen2024m3cotnovelbenchmarkmultidomain">(Chen et al., 2024)</a></p> <p>Techniques for multimodal ‘reasoning’ still lags behind reasoning in text-space by large margin, and seems a promise research avenue.</p> <p>There is a small gap between open source and commercial models such as GPT4 with these results</p>]]></content><author><name></name></author><category term="thoughts"/><category term="reading"/><summary type="html"><![CDATA[Reading list and notes on MLLM]]></summary></entry><entry><title type="html">Clustering in ViT vs. Spikformers</title><link href="https://tpeat.github.io/blog/2024/spikVit/" rel="alternate" type="text/html" title="Clustering in ViT vs. Spikformers"/><published>2024-12-01T17:39:00+00:00</published><updated>2024-12-01T17:39:00+00:00</updated><id>https://tpeat.github.io/blog/2024/spikVit</id><content type="html" xml:base="https://tpeat.github.io/blog/2024/spikVit/"><![CDATA[<p>Redirecting to pdf.</p>]]></content><author><name></name></author><category term="research"/><summary type="html"><![CDATA[Read my recent work on the differences between ViT and spiking ViT]]></summary></entry><entry><title type="html">Why I want to do research</title><link href="https://tpeat.github.io/blog/2024/research/" rel="alternate" type="text/html" title="Why I want to do research"/><published>2024-01-01T15:09:00+00:00</published><updated>2024-01-01T15:09:00+00:00</updated><id>https://tpeat.github.io/blog/2024/research</id><content type="html" xml:base="https://tpeat.github.io/blog/2024/research/"><![CDATA[<p>I always dreamed of becoming on inventor. In 6th grade, I worked closely with an acccelerated learning teacher, Mrs. Smith, studying patents and how to get one. Sadly, ‘inventor’ isn’t a major option at Georgia Tech, so I went with computer engineering due to its relevance to creating a real life Ironman. I switched to CS after one semester because CS enabled me to test my ideas and build things much fasters. Generally speaking, I believe CS is the fastest way to turn an idea into a reality.</p> <p>I like <em>new</em>. I enjoy turning new things inside out until I understand them. I once thought this was a weakness of mine: that I couldn’t ever settle on what sport I wanted to play or what hobby I wanted to engage in. As a child, I would be practicing roller hockey in our coldesac one week, then teaching myself to yoyo or complete a rubix cube, and then I would be grinding basketball drills until I could beat the taller kids. At my high school Senior Night soccer game, the announcer told the entire stadium I was triple majoring in “Math, Economics, and Computer Science” because I couldn’t choose just one.</p> <blockquote> <p>Let’s do an exercise: <em>Describe who you are without talking about your job, school, or hobbies</em>: I am curious, creative, and driven. I am dedicated to lifelong learning, because I want to know more, diversify my perspectives, and understand why things work the way they do. I enjoy exploring, in nature, new experiences, and even complex code/math. I am intune with my imaginitive side and always have an idea. I’m never bored. I am always striving to improve by even just 1% daily. I want the life that is truly life. I recognize that life is hard, and therefore, I enjoy doing hard things. I embrace pain in physical activity, recovery quickly from unfortunate circumstances, and see growth during moments of pain. I love my family and God.</p> </blockquote> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ironman-480.webp 480w,/assets/img/ironman-800.webp 800w,/assets/img/ironman-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ironman.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Ironman" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Because who doesn't love a good visual. </div> <p>I like to think that I have what it takes to be a best in class researcher. I care about big picture things and often ask myself “how can we do this better?”.</p> <p>I have formal training and experience as a software engineer, so I can think in terms of scalability and worst case scenario. This experience is crucial to iteration in my experiments. Soon enough, the only limiting factor in my experiment time will be the ideation and planning phase.</p> <p>I have many questions about machine learning. During my tenure at Georgia Tech, I’ve become quite good at quickly finding solutions to these questions or halting my train of thought when the research direction runs ‘stale’.</p> <h3 id="first-taste">First Taste</h3> <p>My first taste with research was for my Capstone project through the Vertically Integrated Projects (VIP) program at Georgia Tech. The team I joined was Automated Algorithm Design. I applied out of the blue because it seems like the most technical and related to ML. Little did I know that this decision would land me a job at Georgia Tech Research Institute (GTRI) and inspired a career in research.</p> <p>Our project is detailed more in this post: <a href="link">link</a></p> <h3 id="where-am-i-at-now">Where am I at now?</h3> <p>I am currently working at GTRI as a Machine Learning Research Assistant. I work 10-20 hours per week on top of school. Not only do I get paid 🤑, but this opportunity presents a time for me to test my research ideas in an industry setting to improve the efficiency of our machine learning models.</p> <p>I am currently working on improve our model with a DINOv2 backbone, Token Merging, Layer Decay regularization, and a sparser segmentation head. You can read more about that <a href="link">here</a>.</p> <p>I read 3-5 research papers a week and meet with 1-3 PhD students per week. I am working on my experiment design and speeding up my testing.</p> <h3 id="here-are-some-of-my-current-questions">Here are some of my current questions:</h3> <ul> <li>How can I transfer the weights (and therefore knowledge) of a much larger pretrained model into a smaller model of similar architecture without retraining? What about a vastly different architecture landscape?</li> <li>Training architecture: what if I altered a models architecture for 1 forward pass, then altered a models weights for 1 forward/backward pass?</li> <li>Initialize models with weights that have seen both data subsets (ex. two splits of MNIST that contains different digits). If training two models on each dataset, how many passes until it’s forgotten? Can I unlearn a single digit?</li> <li>Dataset generation models: Given just like 3 samples, how can I create millions of them?</li> <li>How could I train an network to interpret images of graphs? Currently, chatGPT can’t even detect a simple cycle in an image.</li> </ul> <p>The list goes on.</p> <h3 id="here-are-some-traps-i-dont-want-to-fall-into">Here are some traps I don’t want to fall into:</h3> <ul> <li>Becoming a corporate slave</li> <li>Research that gets put on the shelf and is never used (after thesis concludes, etc)</li> <li>Burnout</li> </ul> <h3 id="here-is-what-i-want-to-do">Here is what I want to do:</h3> <ul> <li>Answer the important questions</li> <li>Make a lasting impact</li> <li>Become an expert in computer vision, generative AI field</li> <li>Build trust worthy AI that can be deployed in high consequence settings</li> </ul> <h3 id="academic-or-industry">Academic or Industry?</h3> <p>For the next 5 to 10 years my calling is in industry research because I want to see my ideas realized in products that affect many people worldwide. I currently think that Meta’s Foundational AI Research (FAIR) team is one of the best research programs in the world. It is a mission I strive to join and contribute to. I know this will take time and I look forward to the journey in proving my worth to the team.</p> <p>Long term, I aim to move to academic research and teaching. My experience as a teaching assistant was a fufilling one. My favorite part was when students get that ‘lightbulb moment’.</p> <h3 id="the-gameplan">The Gameplan</h3> <p>As of the past 5 months, I have been surveying the field to find what is ‘most important to me’. I want my master’s thesis to be important and I aim to grab a low hanging fruit of a much larger problem that I can tackle for my PhD. I’ve been meeting with PhD students to learn more about their research path and perspective. I am most interested in image/video generation and making state of the art models more efficient.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="research"/><summary type="html"><![CDATA[Some thoughts on why I want to do research]]></summary></entry><entry><title type="html">Survey of Image genAI</title><link href="https://tpeat.github.io/blog/2023/diffusion/" rel="alternate" type="text/html" title="Survey of Image genAI"/><published>2023-12-01T21:01:00+00:00</published><updated>2023-12-01T21:01:00+00:00</updated><id>https://tpeat.github.io/blog/2023/diffusion</id><content type="html" xml:base="https://tpeat.github.io/blog/2023/diffusion/"><![CDATA[<p>Last November, I attended a talk by Dr. Yang Song through ML@GT seminar series on Consistency Models. Inspired by his clear delivery and important research, I felt drawn towards exploring image generation more.</p> <p>Here are some papers I’ve read recently:</p> <h3 id="denoising-diffusion-probabilistic-models">Denoising Diffusion Probabilistic Models</h3> <p>In the landscape of generative artificial intelligence, Denoising Diffusion Probabilistic Models (DDPMs) stand out as a beacon of innovation, merging the worlds of machine learning and thermodynamics in a way that might just make you wonder if a crash course in non-equilibrium thermodynamics is in order. At its core, DDPM operates through a meticulously designed variational bound, which serves as a bridge connecting diffusion probabilistic models with denoising score matching and Langevin dynamics.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ddpm-480.webp 480w,/assets/img/ddpm-800.webp 800w,/assets/img/ddpm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ddpm.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="understanding-the-mechanism">Understanding the Mechanism</h3> <p>DDPMs introduce a progressively lossy decompression scheme, utilizing autoregressive decoding to achieve remarkable results. The essence of diffusion models lies in their structure as parameterized Markov chains, trained through variational inference to master the art of reversing a diffusion process. This technique hinges on learning transitions within the chain, guiding the model to reverse engineer from a state of high entropy back to the original data distribution.</p> <p>One of the pivotal revelations in the study of diffusion models is their parameterization, revealing an equivalence with denoising score matching across multiple noise levels during the training phase. Although these models deliver exceptional sample quality, they encounter challenges with log likelihood when juxtaposed with likelihood-based models, leading researchers to analyze these dynamics through the prism of lossy compression.</p> <h3 id="the-math-behind-the-magic">The Math Behind the Magic</h3> <p>At the heart of diffusion models is a latent variable framework, encapsulated by the equation:</p> <p>’'’insert equation here’’’</p> <p>This equation symbolizes the averaging over all conceivable paths that latent variables could traverse, starting from the original data point and culminating in the learned distribution. The genesis of this journey begins at the “most diffused state,” a point where the data’s inherent structure has been obliterated.</p> <p>The Markov chain that defines the reverse process is a series of learned Gaussian transitions, embarking from a state of maximal entropy. The distinct nature of diffusion models stems from their approximation to the posterior, a fixed Markov chain known as the diffusion process, which incrementally introduces Gaussian noise into the data.</p> <h3 id="forward-pass-simplified">Forward Pass Simplified</h3> <p>Breaking down the forward pass, we see a methodical addition of Gaussian noise at each timestep, transitioning the data from its initial state towards increasing levels of diffusion. This process allows the model to navigate through a multidimensional noise landscape, with each dimension uniformly affected thanks to the identity matrix</p> <p>’'’insert code here’’’</p> <p>However, the real ingenuity emerges in the model’s ability to reparameterize, enabling a tractable closed-form sample through the clever manipulation of variance schedules. Whether adopting a fixed constant or a dynamic schedule, the choice of variance scheduling, from linear to cosine, significantly influences the model’s performance, with recent enhancements spotlighting the efficacy of cosine schedules.</p> <h3 id="reverse-diffusion-the-road-back">Reverse Diffusion: The Road Back</h3> <p>As the model approaches the end of its diffusion journey, the latent variable resembles an isotropic Gaussian, paving the way for the reverse diffusion process. This process involves approximating the less noisy state from a given noisy image, a task elegantly executed by neural networks trained to predict Gaussian parameters.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog-diffusion-480.webp 480w,/assets/img/dog-diffusion-800.webp 800w,/assets/img/dog-diffusion-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/dog-diffusion.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="training-and-beyond">Training and Beyond</h3> <p>Training a DDPM draws parallels with the variational autoencoder (VAE), focusing on optimizing the negative log-likelihood of the training data. This endeavor touches upon concepts like variational lower bounds and Kullback-Leibler divergence, underscoring the intricate dance between achieving tractability and preserving the model’s flexibility to capture the rich structure of arbitrary data.</p> <h3 id="leveraging-physics-for-ai">Leveraging Physics for AI</h3> <p>Interestingly, DDPMs borrow heavily from physics, particularly non-equilibrium statistical physics, to refine their training methodologies. Techniques like Annealed Importance Sampling and Langevin dynamics offer a window into defining Gaussian diffusion processes with target distributions as equilibrium states, showcasing the profound interplay between physics and AI in advancing generative models.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/langevin-dynamics-480.webp 480w,/assets/img/langevin-dynamics-800.webp 800w,/assets/img/langevin-dynamics-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/langevin-dynamics.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="future-directions-and-challenges">Future Directions and Challenges</h3> <p>As we delve deeper into the nuances of DDPMs, questions of efficiency, sample quality, and computational demands come to the fore. The journey from abstract mathematical formulations to practical applications highlights both the potential and the hurdles in harnessing the full power of diffusion models for generative tasks.</p> <h3 id="conclusion">Conclusion</h3> <p>Denoising Diffusion Probabilistic Models represent a fascinating confluence of ideas, from the mathematical intricacies of variational inference to the thermodynamic principles guiding their operation. As we stand on the brink of new discoveries in generative AI, DDPMs offer a promising pathway, challenging us to rethink the boundaries of what’s possible in the realm of artificial intelligence.</p>]]></content><author><name></name></author><category term="cv"/><summary type="html"><![CDATA[Survey of some recent papers on image generation models]]></summary></entry></feed>