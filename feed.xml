<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://tpeat.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tpeat.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-08T02:23:15+00:00</updated><id>https://tpeat.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Why I want to do research</title><link href="https://tpeat.github.io/blog/2024/research/" rel="alternate" type="text/html" title="Why I want to do research"/><published>2024-01-01T15:09:00+00:00</published><updated>2024-01-01T15:09:00+00:00</updated><id>https://tpeat.github.io/blog/2024/research</id><content type="html" xml:base="https://tpeat.github.io/blog/2024/research/"><![CDATA[<p>I always dreamed of becoming on inventor. My journey began in earnest during 6th grade, when I teamed up with Mrs. Carissa Smith for an accelerated learning program, immersing myself in the world of patents for countless hours. Sadly, ‚Äúan inventor‚Äù simply isn‚Äôt a job description or major anymore.</p> <p>This passion led me to Georgia Tech, initially as a Computer Engineering major. My childhood dream of building Ironman was alive and well‚Äîafter all, who wouldn‚Äôt want to create a real-life superhero? However, I quickly realized that Ironman wouldn‚Äôt be complete without his AI counterpart, Jarvis. This revelation steered me toward a major in Computer Science, drawn to the world of machine learning. The field felt like a blend of mystery and magic, captivating my curiosity and creativity.</p> <blockquote> <p>Let‚Äôs do an exercise: <em>Describe who you are without talking about your job, school, or hobbies</em>: I am curious, creative, and thoughtful. I am dedicated to lifelong learning, always striving to improve by even just 1% daily. Respect for others, for Mother Nature, and the roles of brother and son define me as much as any career could.</p> </blockquote> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ironman-480.webp 480w,/assets/img/ironman-800.webp 800w,/assets/img/ironman-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ironman.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Ironman" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Because who doesn't love a good visual. </div> <p>I like to think that I have what it takes to be a best in class researcher. I care about big picture things and often ask myself ‚Äúhow can we do this better?‚Äù.</p> <p>I have formal training and experience as a software engineer, so I can think in terms of scalability and worst case scenario. This experience is crucial to iteration in my experiments. Soon enough, the only limiting factor in my experiment time will be the ideation and planning phase.</p> <p>I have many questions about machine learning. During my tenure at Georgia Tech, I‚Äôve become quite good at quickly finding solutions to these questions or halting my train of thought when the research direction runs ‚Äòstale‚Äô.</p> <h3 id="first-taste">First Taste</h3> <p>My first taste with research was for my Capstone project through the Vertically Integrated Projects (VIP) program at Georgia Tech. The team I joined was Automated Algorithm Design. I applied out of the blue because it seems like the most technical and related to ML. Little did I know that this decision would land me a job at Georgia Tech Research Institute (GTRI) and inspired a career in research.</p> <p>Our project is detailed more in this post: <a href="link">link</a></p> <h3 id="where-am-i-at-now">Where am I at now?</h3> <p>I am currently working at GTRI as a Machine Learning Research Assistant. I work 10-20 hours per week on top of school. Not only do I get paid ü§ë, but this opportunity presents a time for me to test my research ideas in an industry setting to improve the efficiency of our machine learning models.</p> <p>I am currently working on improve our model with a DINOv2 backbone, Token Merging, Layer Decay regularization, and a sparser segmentation head. You can read more about that <a href="link">here</a>.</p> <p>I read 3-5 research papers a week and meet with 1-3 PhD students per week. I am working on my experiment design and speeding up my testing.</p> <h3 id="here-are-some-of-my-current-questions">Here are some of my current questions:</h3> <ul> <li>How can I transfer the weights (and therefore knowledge) of a much larger pretrained model into a smaller model of similar architecture without retraining? What about a vastly different architecture landscape?</li> <li>Training architecture: what if I altered a models architecture for 1 forward pass, then altered a models weights for 1 forward/backward pass?</li> <li>Initialize models with weights that have seen both data subsets (ex. two splits of MNIST that contains different digits). If training two models on each dataset, how many passes until it‚Äôs forgotten? Can I unlearn a single digit?</li> <li>Dataset generation models: Given just like 3 samples, how can I create millions of them?</li> <li>How could I train an network to interpret images of graphs? Currently, chatGPT can‚Äôt even detect a simple cycle in an image.</li> </ul> <p>The list goes on.</p> <h3 id="here-are-some-traps-i-dont-want-to-fall-into">Here are some traps I don‚Äôt want to fall into:</h3> <ul> <li>Becoming a corporate slave</li> <li>Research that gets put on the shelf and is never used (after thesis concludes, etc)</li> <li>Burnout</li> </ul> <h3 id="here-is-what-i-want-to-do">Here is what I want to do:</h3> <ul> <li>Answer the important questions</li> <li>Make a lasting impact</li> <li>Become an expert in computer vision, generative AI field</li> <li>Build trust worthy AI that can be deployed in high consequence settings</li> </ul> <h3 id="academic-or-industry">Academic or Industry?</h3> <p>For the next 5 to 10 years my calling is in industry research because I want to see my ideas realized in products that affect many people worldwide. I currently think that Meta‚Äôs Foundational AI Research (FAIR) team is one of the best research programs in the world. It is a mission I strive to join and contribute to. I know this will take time and I look forward to the journey in proving my worth to the team.</p> <p>Long term, I aim to move to academic research and teaching. My experience as a teaching assistant was a fufilling one. My favorite part was when students get that ‚Äòlightbulb moment‚Äô.</p> <h3 id="the-gameplan">The Gameplan</h3> <p>As of the past 5 months, I have been surveying the field to find what is ‚Äòmost important to me‚Äô. I want my master‚Äôs thesis to be important and I aim to grab a low hanging fruit of a much larger problem that I can tackle for my PhD. I‚Äôve been meeting with PhD students to learn more about their research path and perspective. I am most interested in image/video generation and making state of the art models more efficient.</p>]]></content><author><name></name></author><category term="thoughts"/><summary type="html"><![CDATA[Some thoughts on why I want to do research]]></summary></entry><entry><title type="html">Survey of Image genAI</title><link href="https://tpeat.github.io/blog/2023/diffusion/" rel="alternate" type="text/html" title="Survey of Image genAI"/><published>2023-12-01T21:01:00+00:00</published><updated>2023-12-01T21:01:00+00:00</updated><id>https://tpeat.github.io/blog/2023/diffusion</id><content type="html" xml:base="https://tpeat.github.io/blog/2023/diffusion/"><![CDATA[<p>Last November, I attended a talk by Dr. Yang Song through ML@GT seminar series on Consistency Models. Inspired by his clear delivery and important research, I felt drawn towards exploring image generation more.</p> <p>Here are some papers I‚Äôve read recently:</p> <h3 id="denoising-diffusion-probabilistic-models">Denoising Diffusion Probabilistic Models</h3> <p>In the landscape of generative artificial intelligence, Denoising Diffusion Probabilistic Models (DDPMs) stand out as a beacon of innovation, merging the worlds of machine learning and thermodynamics in a way that might just make you wonder if a crash course in non-equilibrium thermodynamics is in order. At its core, DDPM operates through a meticulously designed variational bound, which serves as a bridge connecting diffusion probabilistic models with denoising score matching and Langevin dynamics.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ddpm-480.webp 480w,/assets/img/ddpm-800.webp 800w,/assets/img/ddpm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ddpm.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="understanding-the-mechanism">Understanding the Mechanism</h3> <p>DDPMs introduce a progressively lossy decompression scheme, utilizing autoregressive decoding to achieve remarkable results. The essence of diffusion models lies in their structure as parameterized Markov chains, trained through variational inference to master the art of reversing a diffusion process. This technique hinges on learning transitions within the chain, guiding the model to reverse engineer from a state of high entropy back to the original data distribution.</p> <p>One of the pivotal revelations in the study of diffusion models is their parameterization, revealing an equivalence with denoising score matching across multiple noise levels during the training phase. Although these models deliver exceptional sample quality, they encounter challenges with log likelihood when juxtaposed with likelihood-based models, leading researchers to analyze these dynamics through the prism of lossy compression.</p> <h3 id="the-math-behind-the-magic">The Math Behind the Magic</h3> <p>At the heart of diffusion models is a latent variable framework, encapsulated by the equation:</p> <p>‚Äô'‚Äôinsert equation here‚Äô‚Äô‚Äô</p> <p>This equation symbolizes the averaging over all conceivable paths that latent variables could traverse, starting from the original data point and culminating in the learned distribution. The genesis of this journey begins at the ‚Äúmost diffused state,‚Äù a point where the data‚Äôs inherent structure has been obliterated.</p> <p>The Markov chain that defines the reverse process is a series of learned Gaussian transitions, embarking from a state of maximal entropy. The distinct nature of diffusion models stems from their approximation to the posterior, a fixed Markov chain known as the diffusion process, which incrementally introduces Gaussian noise into the data.</p> <h3 id="forward-pass-simplified">Forward Pass Simplified</h3> <p>Breaking down the forward pass, we see a methodical addition of Gaussian noise at each timestep, transitioning the data from its initial state towards increasing levels of diffusion. This process allows the model to navigate through a multidimensional noise landscape, with each dimension uniformly affected thanks to the identity matrix</p> <p>‚Äô'‚Äôinsert code here‚Äô‚Äô‚Äô</p> <p>However, the real ingenuity emerges in the model‚Äôs ability to reparameterize, enabling a tractable closed-form sample through the clever manipulation of variance schedules. Whether adopting a fixed constant or a dynamic schedule, the choice of variance scheduling, from linear to cosine, significantly influences the model‚Äôs performance, with recent enhancements spotlighting the efficacy of cosine schedules.</p> <h3 id="reverse-diffusion-the-road-back">Reverse Diffusion: The Road Back</h3> <p>As the model approaches the end of its diffusion journey, the latent variable resembles an isotropic Gaussian, paving the way for the reverse diffusion process. This process involves approximating the less noisy state from a given noisy image, a task elegantly executed by neural networks trained to predict Gaussian parameters.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dog-diffusion-480.webp 480w,/assets/img/dog-diffusion-800.webp 800w,/assets/img/dog-diffusion-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/dog-diffusion.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="training-and-beyond">Training and Beyond</h3> <p>Training a DDPM draws parallels with the variational autoencoder (VAE), focusing on optimizing the negative log-likelihood of the training data. This endeavor touches upon concepts like variational lower bounds and Kullback-Leibler divergence, underscoring the intricate dance between achieving tractability and preserving the model‚Äôs flexibility to capture the rich structure of arbitrary data.</p> <h3 id="leveraging-physics-for-ai">Leveraging Physics for AI</h3> <p>Interestingly, DDPMs borrow heavily from physics, particularly non-equilibrium statistical physics, to refine their training methodologies. Techniques like Annealed Importance Sampling and Langevin dynamics offer a window into defining Gaussian diffusion processes with target distributions as equilibrium states, showcasing the profound interplay between physics and AI in advancing generative models.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/langevin-dynamics-480.webp 480w,/assets/img/langevin-dynamics-800.webp 800w,/assets/img/langevin-dynamics-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/langevin-dynamics.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="future-directions-and-challenges">Future Directions and Challenges</h3> <p>As we delve deeper into the nuances of DDPMs, questions of efficiency, sample quality, and computational demands come to the fore. The journey from abstract mathematical formulations to practical applications highlights both the potential and the hurdles in harnessing the full power of diffusion models for generative tasks.</p> <h3 id="conclusion">Conclusion</h3> <p>Denoising Diffusion Probabilistic Models represent a fascinating confluence of ideas, from the mathematical intricacies of variational inference to the thermodynamic principles guiding their operation. As we stand on the brink of new discoveries in generative AI, DDPMs offer a promising pathway, challenging us to rethink the boundaries of what‚Äôs possible in the realm of artificial intelligence.</p>]]></content><author><name></name></author><category term="cv"/><summary type="html"><![CDATA[Survey of some recent papers on image generation models]]></summary></entry></feed>