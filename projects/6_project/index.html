<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Messi-Anything | Tristan Peat </title> <meta name="author" content="Tristan Peat"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tpeat.github.io/projects/6_project/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tristan </span> Peat </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Messi-Anything</h1> <p class="post-description"></p> </header> <article> <h2 id="project-goal">Project Goal</h2> <p>Create an end to end model that take an RGB image, segments out humans if they exist, and inpaints the space with a generated image of Messi.</p> <p>The original name was Segment-And-Replace, but we thought something a little more fun, hence Messi everything.</p> <p>How did this idea come about?</p> <ul> <li>I have experience with segmentation and deep vision transformers</li> <li>I want to learn more about image generation models</li> <li>I play soccer and my favorite player is Messi</li> </ul> <hr> <h2 id="combining-the-latent-space">Combining the latent space:</h2> <p>My idea was simple, we spend so much time encoding the input images into a latent space, we can leverage work already performed by the segmentation model when creating the masks to aide the gen model.</p> <p>I figured because the Hiera encoder reaches a smaller spatial dimension quicker than the generative UNet, with dimensions (256x256 –&gt; 64x64 –&gt; 32x32 –&gt; 16x16 –&gt; 8x8) that we could perform cross attention between the lower dimension space without too much overhead</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model-arch-480.webp 480w,/assets/img/model-arch-800.webp 800w,/assets/img/model-arch-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/model-arch.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>You may have already noted that the dimension don’t match even closely: trying to perform cross attention between (32x32x192) and (128x128x512) tensors is a tricky task. Projecting the tensors channels into a similar dimension is a well-known tactic, but what to do about the spatial dimension? I could interpolate the smallest to match the largest, or downsample the largest to match the smallest, some combination of both?</p> <hr> <h2 id="data">Data</h2> <p>For the segmentation model, I wanted to use a well established and easily available dataset. I chose MSCOCO because it contains a decent amount of ‘people’ labels. I wrote this quick script to download the instance json, filter by people, and then download associated masks and labels.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_people</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># avoid overcrowing of people in the images
</span>
<span class="n">coco</span> <span class="o">=</span> <span class="nc">COCO</span><span class="p">(</span><span class="n">annFile</span><span class="p">)</span>

<span class="c1"># # get category ids for everythign containing person
</span><span class="n">catIds</span> <span class="o">=</span> <span class="n">coco</span><span class="p">.</span><span class="nf">getCatIds</span><span class="p">(</span><span class="n">catNms</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">person</span><span class="sh">'</span><span class="p">])</span>
<span class="c1"># get images for person category
</span><span class="n">imgIds</span> <span class="o">=</span> <span class="n">coco</span><span class="p">.</span><span class="nf">getImgIds</span><span class="p">(</span><span class="n">catIds</span><span class="o">=</span><span class="n">catIds</span><span class="p">)</span>

<span class="k">for</span> <span class="n">imgId</span> <span class="ow">in</span> <span class="n">imgIds</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">coco</span><span class="p">.</span><span class="nf">loadImgs</span><span class="p">(</span><span class="n">imgId</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">annIds</span> <span class="o">=</span> <span class="n">coco</span><span class="p">.</span><span class="nf">getAnnIds</span><span class="p">(</span><span class="n">imgIds</span><span class="o">=</span><span class="n">img</span><span class="p">[</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">],</span> <span class="n">catIds</span><span class="o">=</span><span class="n">catIds</span><span class="p">,</span> <span class="n">iscrowd</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="n">annotations</span> <span class="o">=</span> <span class="n">coco</span><span class="p">.</span><span class="nf">loadAnns</span><span class="p">(</span><span class="n">annIds</span><span class="p">)</span>
    <span class="c1"># this avoids crowded frames
</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">annotations</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_people</span><span class="p">:</span>
        <span class="k">continue</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="sh">'</span><span class="s">coco_url</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="nc">BytesIO</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">content</span><span class="p">))</span>
    <span class="n">image</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">dataDir</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">dataType</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">img</span><span class="p">[</span><span class="sh">"</span><span class="s">file_name</span><span class="sh">"</span><span class="p">]</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">annotations</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">img</span><span class="p">[</span><span class="sh">'</span><span class="s">height</span><span class="sh">'</span><span class="p">],</span> <span class="n">img</span><span class="p">[</span><span class="sh">'</span><span class="s">width</span><span class="sh">'</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">ann</span> <span class="ow">in</span> <span class="n">annotations</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">+=</span> <span class="n">coco</span><span class="p">.</span><span class="nf">annToMask</span><span class="p">(</span><span class="n">ann</span><span class="p">)</span>
        <span class="c1"># save mask ...
</span></code></pre></div></div> <p></p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mscoco-ex-480.webp 480w,/assets/img/mscoco-ex-800.webp 800w,/assets/img/mscoco-ex-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/mscoco-ex.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example images from MS coco, the furthest right would not pass the max_people threshold </div> <p>For the Messi dataset, the team found a few Messi datasets online from kaggle and parsed them together. The final count was 1087 images. The resolutions and dimensions of the images were diverse and wildly different.</p> <p>Note that there are no labels associated with the data required for the generative model, making the generative part of our model fully unsupervised. This is because the model is merely learning the distribution of the input images, so that it can map a noise vector to a sample from the distribution that is novel and realistic.</p> <hr> <h2 id="closer-look-at-the-segmentation-model">Closer Look at the Segmentation Model</h2> <p>The encoder is based on the Hiera model introduced by Dan Bolya and Meta. The model is about 53M paramters and I loaded pretrained weights adapted from DINOv2 (also product of Meta).</p> <p>The decoder is an extremly simple Feature Pyramid Network (FPN) that has 3 blocks of upsampling and intepolation from</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>8x8[input] -&gt; 16x16 -&gt; 32x32 -&gt; 64x64
</code></pre></div></div> <p>Then, I added transformer blocks (from the timm library) in between the encoder and decoder to create a more powerful representation in the latent space.</p> <p>The combined model looks like so:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nf">create_hiera_model</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
            <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">self</span><span class="p">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span>
            <span class="nc">Block</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="nc">FPNSegmentationHead</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">decode_intermediate_input</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">shortcut_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">96</span><span class="p">,</span><span class="mi">192</span><span class="p">,</span><span class="mi">384</span><span class="p">,</span><span class="mi">768</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">intermediates</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">return_intermediates</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">shortcuts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">intermediates</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">intermediates</span><span class="p">:</span>
            <span class="n">shortcuts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">blocks</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">([</span><span class="n">x</span><span class="p">],</span> <span class="n">shortcuts</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h3 id="training-details">Training Details</h3> <p>With access to the Georgia Tech PACE-Ice super compute cluster, we were able to train the segmentation model for 200 epochs on a single H100 with 80GB RAM.</p> <p>The initial learning = 0.001 and optimizer was Adam.</p> <p>A variety of loss function were tested for best results but ultimately only BCELossWithLogits was used.</p> <p>Other variations include IoULoss using a SoftJaccardIndex and DiceLoss</p> <p>Note: that IOU = Intersection over union which can be done in pytorch simply using:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">intersection</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span> <span class="o">*</span> <span class="n">target</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="p">(</span><span class="n">union</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">+</span> <span class="n">target</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">mean_iou</span> <span class="o">=</span> <span class="p">(</span><span class="n">intersection</span> <span class="o">/</span> <span class="n">union</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div> <p>Naturally you will want to add in small constants for numeric stability and be cautious of the dimension of your tensors.</p> <p>Iou ranges between 0-1 with 1 being perfect.</p> <p>To turn Iou into a loss function, you need only subtract iou from 1</p> <p>Below you will find training curves for 100 epochs.</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bce_loss-480.webp 480w,/assets/img/bce_loss-800.webp 800w,/assets/img/bce_loss-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/bce_loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/iou-per-epoch-480.webp 480w,/assets/img/iou-per-epoch-800.webp 800w,/assets/img/iou-per-epoch-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/iou-per-epoch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> BCE Loss on the left with IOU for eval in the right </div> <h3 id="evaluation-details">Evaluation Details</h3> <p>A test set of 100 images was withheld from training. Below is the performance on a image from the test set.</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/test-messi-480.webp 480w,/assets/img/test-messi-800.webp 800w,/assets/img/test-messi-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/test-messi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example output on test image </div> <p>You may notice the choppy edges on the edge of the mask. This is the result of the FPN only have 3 upsampling convolution + interpolation blocks, so the final output from the FPN is only 64x64, then the rest is interpolated (makes blocky represenation).</p> <p>The final convolution back to the original dimension was removed for inference speed.</p> <hr> <h2 id="closer-look-at-consistency-models">Closer look at Consistency Models</h2> <p>The idea of consistency models originates from Dr. Yang Song, from OpenAI.</p> <p>NOTE: mid project, I switched from A100 GPUs to H100 GPUs because there were 8 times at many H100s as A100s. This meant I had to find a mapping between the modules such as cuda available on A100 to the one’s available on H100. Furthermore, I had to recreate my environment with these new modules and thus I upgraded from <code class="language-plaintext highlighter-rouge">pytorch=2.0.1</code> to <code class="language-plaintext highlighter-rouge">pytorch=2.1.0</code>. This meant that the version of <code class="language-plaintext highlighter-rouge">flash-attn</code> in the <code class="language-plaintext highlighter-rouge">setup.py</code> file for the consistency-models was out of date and the FlashAttention mechanisms in the UNet would no longer work. The speedup offered by hardware efficient attention is signfigant to I wrote a fix:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TristanAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Implementation of F.scaled_... for flasha ttention
    using pytorch &gt; 2.0, they have legacy version outdated for current cuda version

    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">embed_dim</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">causal</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="kn">from</span> <span class="n">einops</span> <span class="kn">import</span> <span class="n">rearrange</span>

        <span class="k">assert</span> <span class="n">batch_first</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">device</span><span class="sh">"</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="sh">"</span><span class="s">dtype</span><span class="sh">"</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>

        <span class="nf">assert </span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="sh">"</span><span class="s">self.kdim must be divisible by num_heads</span><span class="sh">"</span>
        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="sh">"</span><span class="s">Only support head_dim == 16, 32, or 64</span><span class="sh">"</span>

        <span class="n">self</span><span class="p">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rearrange</span> <span class="o">=</span> <span class="n">rearrange</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">qkv</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">need_weights</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">rearrange</span><span class="p">(</span>
            <span class="n">qkv</span><span class="p">,</span> <span class="sh">"</span><span class="s">b (three h d) s -&gt; b s three h d</span><span class="sh">"</span><span class="p">,</span> <span class="n">three</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">num_heads</span>
        <span class="p">)</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">key</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">value</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># print(query.shape)
</span>        <span class="k">with</span> <span class="n">th</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">sdp_kernel</span><span class="p">(</span>
            <span class="n">enable_flash</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">enable_math</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">enable_mem_efficient</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">)</span> <span class="ow">and</span> <span class="n">th</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="nf">autocast</span><span class="p">():</span>
            <span class="c1"># th.cuda.synchronize() # questionable call to sync, Don't want to force all models to schedule this
</span>            <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">attention_dropout</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">causal</span><span class="p">)</span>
            <span class="c1"># print("out shape:", out.shape)
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="sh">"</span><span class="s">b s h d -&gt; b (h d) s</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>Then I overrode the Attention module select when <code class="language-plaintext highlighter-rouge">use_flash=True</code> in the Unet.</p> <p>As detailed in the diagram above, the UNet architecture relies on a combination of ResNet and Attention blocks in sequence.</p> <p>Attention blocks are only used when in the <code class="language-plaintext highlighter-rouge">attention_resolution</code> list, which is typically 32 or below. This is done so that we aren’t wasting compute on computing attention (which is already an $O(n^2)$ operation) on tensors with large spatial dimensions.</p> <p>Therefore, most of the model is of the form: ResNetBlock -&gt; ResNetBlock -&gt; ResNetBlockWithDownsampling, or vice versa when in the decoder part of the network.</p> <h3 id="training-details-1">Training details</h3> <p>As mentioned earlier, the dataset consists of 1087 images of Messi.</p> <p>I created two models of different resolutions but <code class="language-plaintext highlighter-rouge">CUDA OOM</code> errors forced me to make two other tweaks:</p> <p>Image A:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image_size = 256
model_dimension = 256
batch_size = 16
</code></pre></div></div> <p>Image B:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image_size = 512
model_dimension = 192
batch_size = 4
</code></pre></div></div> <p>I trained both models from scratch using a exponential moving average (EMA) with a teacher model, student model, and 3 versions of EMA models tracking models over a number of historical steps.</p> <p>I trained on a single H100 80GB GPU with the following environment variables:</p> <pre><code class="language-pytyhon">python = 3.10
gcc = 12.3
openmpi = 4.1.5
cuda = 12.1.1
pytorch = 2.1.0
</code></pre> <p>The training lasted for 16 hours and in that time model A reached 100k steps which accounting for its batch_size means it saw 1.6M images and model B reached 150k steps means its saw 600k images.</p> <hr> <h2 id="consistency-model-results">Consistency Model Results</h2> <p>The typical evaluation metric for quality of generative image models is Frechet Inception Distance (FID). FID was calculated using the <code class="language-plaintext highlighter-rouge">pytorch-fid</code> library.</p> <p></p> <div class="row justify-content-sm-center"> <table data-toggle="table" data-url="/assets/json/table_data.json"> <thead> <tr> <th data-field="steps">Steps</th> <th data-field="fid">FID</th> </tr> </thead> </table> </div> <p></p> <p>Note: the <code class="language-plaintext highlighter-rouge">pytorch-fid</code> library requires that you use at least <code class="language-plaintext highlighter-rouge">2048</code> images for evaluation, both in original dataset and generated dataset. However, I only used 1087 for training. Therefore, the naive fix was to create copies of the images until I met the target count. However, this meant that I should also only sample 1087 from the generative model and then also create copies??? Idk</p> <p></p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/best_ex_1-480.webp 480w,/assets/img/best_ex_1-800.webp 800w,/assets/img/best_ex_1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/best_ex_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/best_ex_2-480.webp 480w,/assets/img/best_ex_2-800.webp 800w,/assets/img/best_ex_2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/best_ex_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/best_ex_3-480.webp 480w,/assets/img/best_ex_3-800.webp 800w,/assets/img/best_ex_3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/best_ex_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/best_ex_4-480.webp 480w,/assets/img/best_ex_4-800.webp 800w,/assets/img/best_ex_4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/best_ex_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/best_ex_5-480.webp 480w,/assets/img/best_ex_5-800.webp 800w,/assets/img/best_ex_5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/best_ex_5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/best_ex_6-480.webp 480w,/assets/img/best_ex_6-800.webp 800w,/assets/img/best_ex_6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/best_ex_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/best_ex_7-480.webp 480w,/assets/img/best_ex_7-800.webp 800w,/assets/img/best_ex_7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/best_ex_7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/best_ex_8-480.webp 480w,/assets/img/best_ex_8-800.webp 800w,/assets/img/best_ex_8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/best_ex_8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/best_ex_10-480.webp 480w,/assets/img/best_ex_10-800.webp 800w,/assets/img/best_ex_10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/best_ex_10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Visual inspection shows that its got a lot of things right, and obvious things to improve upon. That is, you can tell its messi. It’s got the Argentina and Barcelona kits down,mabye it’s the pretty similar stripe pattern. You can see glimpses of the new pink Inter Miami kit in many of the sample too.</p> <p>Ears, hands and different poses will always be difficult for generation models and I think mine needs a few hundred thousand more epochs before it starts to get this down.</p> <p>For this small selection of “best” images, I went through the original dataset and find examples close to them and couldn’t find much, which is a good sign that its truly syntehsizing across the entire distribution rather than just outputing the training data.</p> <hr> <h2 id="limitations-and-feedback">Limitations and feedback:</h2> <ul> <li> <strong>Noisy data:</strong> Ah yes, a tale as old as time. A machine learning researcher so focused on his model, that he doesn’t invest in his data until its too late… that’s not me. Look at some of my favorite examples I found in the training set:</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/not_even_messi-480.webp 480w,/assets/img/not_even_messi-800.webp 800w,/assets/img/not_even_messi-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/not_even_messi.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/not_even_messi_2-480.webp 480w,/assets/img/not_even_messi_2-800.webp 800w,/assets/img/not_even_messi_2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/not_even_messi_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/barely_messi-480.webp 480w,/assets/img/barely_messi-800.webp 800w,/assets/img/barely_messi-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/barely_messi.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Limited storgage.</strong> My storage use on the PACE-ICE computer cluster is limited to 300GB, thus when checkpointing every 5k iterations to ensure I don’t lose valuable hours if the training fails by making backups often, I run out of storage in 150k iterations or 15 checkpoints (each checkpoint 2GB for 5 state dicts, assuming I start at 50% storage with dataset and other files consuming the system). This turns out to be one the largest detriments to the project because I could not have concurrent runs because it would fill the storage even quicker. Furthermore, all these problems are only for the generative model! We still needed to train a segmentation model.</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pace-storage-480.webp 480w,/assets/img/pace-storage-800.webp 800w,/assets/img/pace-storage-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pace-storage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Broken MPI modules.</strong> My model would simply hang when I tried to allocate more than one GPU and use MPI to coordinate the distribution of model and input data.</li> <li> <strong>Limited access to compute.</strong> It was a constant battle tring to acquire a large enough GPU for my purposes. When loading the teacher and student model, optimizer, and then pulling data, I was getting dangerously close to the 80GB datalimit on the largest GPUs that I could get for free. This raises many questions about how industry handles such large models.</li> <li> <strong>512 pixel resolution was just too big.</strong> After 100k iterations, the model was only starting to form the notion of a face in the output.</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/512x512-480.webp 480w,/assets/img/512x512-800.webp 800w,/assets/img/512x512-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/512x512.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Resizing instead of cropping!</strong> Naively, I designed the FID score to resize all of the original images to be 64x64 so that they could be evaluated against the image generated by the model. Here’s why this was a bad idea:</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/messi_small_1-480.webp 480w,/assets/img/messi_small_1-800.webp 800w,/assets/img/messi_small_1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/messi_small_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/messi_small_2-480.webp 480w,/assets/img/messi_small_2-800.webp 800w,/assets/img/messi_small_2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/messi_small_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/messi_small_3-480.webp 480w,/assets/img/messi_small_3-800.webp 800w,/assets/img/messi_small_3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/messi_small_3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>As you can see, the dimensions of Messi are all out of wack. However, this issue was corrected by the final.</p> <ul> <li> <strong>Limitations of center crop!</strong> Performing center crop fixes the dimension warp, but introduces issues where we miss Messi’s face and wind up with some random focal point.</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bad_crop_1-480.webp 480w,/assets/img/bad_crop_1-800.webp 800w,/assets/img/bad_crop_1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/bad_crop_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bad_crop_2-480.webp 480w,/assets/img/bad_crop_2-800.webp 800w,/assets/img/bad_crop_2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/bad_crop_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bad_crop_3-480.webp 480w,/assets/img/bad_crop_3-800.webp 800w,/assets/img/bad_crop_3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/bad_crop_3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Pytorch-FID calculations</strong>: Pytorch-FID intrinsically requires a minimum of 2048 images. Makes sense. You wouldn’t want your eval code to be judging similarity between distributions based on samples that are too small. Problem is: our original dataset was less than 2048, it was 1087 images of Messi. Quick fix: I made duplicates of some images in the dataset until I hit that target_count. At first I did a similar thing with the generative images, however starting from only 500 samples. Meaning if those 500 samples contained some real duds, they were being duplicated multiple times but also if I had a really good batch, they were unfairly weighted against the original sample. Sampling with a multi-step approach (40 steps) does take time (~ 1 sample/sec at 256 resolution), so I tried to cheat the system by requiring it to only make 1087 sample, then call the sample duplicate_image function that I used on the original dataset.</li> </ul> <h2 id="inpainting">Inpainting</h2> <p>The inpainting script provided in the constiency model repo was insufficient for my purposes, because it randomly creates some rectangle in an image. But I want to use masks from the segmentation model to tell the consistency model where to fill.</p> <p>My first attempt got suprisingly good results, I could see the human face starting to form.</p> <p>I soon realized it was uncovering features that were present in the original image, which the consitency model should have no knowledge of, so I caught my bug.</p> <p>I was created my initial noise vector from the original image, but rather it should be drawn from the generator.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_T</span> <span class="o">=</span> <span class="n">generator</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">args</span><span class="p">.</span><span class="n">sigma_max</span>
</code></pre></div></div> <p>Then the inpainting function makes a forward pass through the consistency model and adds the preciction only the part where there is no mask as noted by <code class="language-plaintext highlighter-rouge">x1 * (1 - masks)</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">replacement</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">):</span>
        <span class="n">x_mix</span> <span class="o">=</span> <span class="n">x0</span> <span class="o">*</span> <span class="n">masks</span> <span class="o">+</span> <span class="n">x1</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">masks</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_mix</span>

<span class="n">images</span> <span class="o">=</span> <span class="nf">replacement</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="o">-</span><span class="n">th</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">images</span><span class="p">))</span>

<span class="c1"># Convert the time schedule based on the given rho
</span><span class="n">t_max_rho</span> <span class="o">=</span> <span class="n">t_max</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">rho</span><span class="p">)</span>
<span class="n">t_min_rho</span> <span class="o">=</span> <span class="n">t_min</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">rho</span><span class="p">)</span>
<span class="n">s_in</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">new_ones</span><span class="p">([</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_max_rho</span> <span class="o">+</span> <span class="n">ts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">t_min_rho</span> <span class="o">-</span> <span class="n">t_max_rho</span><span class="p">))</span> <span class="o">**</span> <span class="n">rho</span>
    <span class="c1"># call to model
</span>    <span class="n">x0</span> <span class="o">=</span> <span class="nf">distiller</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span> <span class="o">*</span> <span class="n">s_in</span><span class="p">)</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="c1"># inpaint the image with prediction
</span>    <span class="n">x0</span> <span class="o">=</span> <span class="nf">replacement</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
    <span class="n">next_t</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_max_rho</span> <span class="o">+</span> <span class="n">ts</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">t_min_rho</span> <span class="o">-</span> <span class="n">t_max_rho</span><span class="p">))</span> <span class="o">**</span> <span class="n">rho</span>
    <span class="n">next_t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">next_t</span><span class="p">,</span> <span class="n">t_min</span><span class="p">,</span> <span class="n">t_max</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">generator</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">next_t</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">t_min</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">images</span>
</code></pre></div></div> <p>But, I was getting very <em>colorful</em> results!</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/inpaint_v0-480.webp 480w,/assets/img/inpaint_v0-800.webp 800w,/assets/img/inpaint_v0-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/inpaint_v0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/inpaint_v2-480.webp 480w,/assets/img/inpaint_v2-800.webp 800w,/assets/img/inpaint_v2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/inpaint_v2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left is original image minus its mask, right is sample ouputs from the inpainter </div> <p>Has to be a normalization issue, right?</p> <h2 id="appendix">Appendix</h2> <p>Space for other funny images</p> <p>Check out this picasso!</p> <div class="row justify-content-sm-center"> <div class="col-lg mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/messi-picasso-480.webp 480w,/assets/img/messi-picasso-800.webp 800w,/assets/img/messi-picasso-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/messi-picasso.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Tristan Peat. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>