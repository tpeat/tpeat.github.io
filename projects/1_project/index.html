<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Efficient Object Tracking | Tristan Peat </title> <meta name="author" content="Tristan Peat"> <meta name="description" content="Tracking airborne objects with Associating Objects with Transformers model"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tpeat.github.io/projects/1_project/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tristan </span> Peat </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Efficient Object Tracking</h1> <p class="post-description">Tracking airborne objects with Associating Objects with Transformers model</p> </header> <article> <hr> <h3 id="project-objective">Project Objective:</h3> <ul> <li>Track single a airborne object from closing distances</li> <li>Leverage transfer learning and finetune on a custom airborne object dataset</li> <li>Model must handle states of occlusion/deformity/clutter/multi-object</li> <li>Explore state of the art tracking techniques</li> <li>Experiment with reinforcement learning based trackers</li> <li>Model must fit into a single GPU, use &lt;12G of RAM, and operate at &gt;50FPS for inference time</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/track-tmp-480.webp 480w,/assets/img/track-tmp-800.webp 800w,/assets/img/track-tmp-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/track-tmp.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="key-contributions">Key contributions:</h3> <ul> <li>Found Amazon Airborn Object Tracking dataset</li> <li>Implemented custom dataloader, downloading images from S3, cropping, and segmenting the images into Annotation binary masks</li> <li>Conducted full literature review, ultimately choosing Associating Objects with Transformers model architecture as launching point</li> <li>Identified key areas of improvement for the model such as the need for a better backbone, reducing history of flights, and using a sparser segmentation head</li> <li>Developed hierarchical DINO based encoder backbone</li> <li>Leverage TokenMerging to improve efficiency of attention mechanisms by reducing token number without loss of information</li> <li>Introduced FlashAttention for enhanced GPU usage</li> <li>Launched hundreds of experiments on multi GPU, slurm cluster</li> </ul> <p>More details below on the progress from literature review, dataset choice, augmentations, manipulations, model choice, training architecture, next steps.</p> <hr> <h3 id="literature-review">Literature Review</h3> <p>The initial literature review was focused on learning from state of the art tracking techniques as well as identifying the feasibility of using RL for tracking. RL in tracking is not well studied and typicaly deep learning methods are preferred for their quicker inference speeds.</p> <p>Key takeaways:</p> <ul> <li>One stage networks are preferred for end to end training and faster inference time</li> <li>IoU and distance from centroid are preferred metrics</li> <li>Segmentation is preffered for objects of varying shape, but is slower</li> <li>Many models focus on multi-object tracking (MOT) and also perform worse on single-object tracking (SOT)</li> <li>LaSOT is most common SOT dataset, VOT2018, GOT-10k honorable mention</li> <li>Template matching is extremely fast, Siamese networks use this week to balance speed and accuracy</li> </ul> <p><strong>Complex Environments and Object Variability:</strong> Effective tracking in distracting environments necessitates handling objects with large variance in shape and scale, and coping with both partial and full occlusions.</p> <p><strong>Model Exploration:</strong></p> <p>Template matching <a class="citation" href="#hu2022siammask">(Hu et al., 2022)</a></p> <p>Why we didn’t use RL:</p> <p>What makes DINO such a good encoder?</p> <p>How should we handle memory of past states/trajectory?</p> <p><strong>Self-Supervised Learning and Transformers:</strong> The exploration of self-supervised learning models like DINO, and the integration of Transformers, suggests a shift towards leveraging these advanced architectures for improved tracking performance, especially in understanding long-range dependencies and spatial-temporal relationships.</p> <h3 id="data-sourcing-amazon-airborne-object-tracking-dataset">Data sourcing: Amazon Airborne Object Tracking Dataset</h3> <p>The Airborne object tracking dataset consists of 164 hours of flight data, 4943 flight sequences of aroudn 120 seconds each. This equates to over 3.3M images with annotations of 2d bounding box, object class, and distance.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/obj-track-challenge-480.webp 480w,/assets/img/obj-track-challenge-800.webp 800w,/assets/img/obj-track-challenge-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/obj-track-challenge.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tracking2-480.webp 480w,/assets/img/tracking2-800.webp 800w,/assets/img/tracking2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tracking2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Challenge logo and samples from dataset </div> <h3 id="converting-bounding-box-to-segmentation-mask">Converting bounding box to segmentation mask</h3> <p>Enter Segmant Anything model.</p> <p>By passing the image along with its respective bounding box to the “Segment Anything” model, it’s possible to obtain precise segmentation masks that outline the exact contours of objects within an image. This model leverages advanced deep learning techniques to understand and delineate the object’s shape, going beyond the limitations of bounding boxes to provide a pixel-perfect representation. This method not only improves the accuracy of object tracking, especially in complex scenes with overlapping objects, but also facilitates a range of applications that require detailed object shapes, such as advanced image editing, augmented reality, and more sophisticated scene understanding tasks.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/segment-480.webp 480w,/assets/img/segment-800.webp 800w,/assets/img/segment-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/segment.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Examples from the Segment Anything model. </div> <p>The Segmant Anything model is extremely performant on most images but fails on some with extremly small targets, typically outputting the entire bbox as a annotation mask or outputting no annotation <a class="citation" href="#kirillov2023segment">(Kirillov et al., 2023)</a>.</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/helicopter-480.webp 480w,/assets/img/helicopter-800.webp 800w,/assets/img/helicopter-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/helicopter.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/copter-seg-480.webp 480w,/assets/img/copter-seg-800.webp 800w,/assets/img/copter-seg-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/copter-seg.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> SAM model applied to sample from Airborne Object Tracking dataset. </div> <h2 id="model-architecture">Model Architecture</h2> <p>The chosen architecture was based on the <a href="https://arxiv.org/pdf/2106.02638.pdf" rel="external nofollow noopener" target="_blank">Associating Objects with Transformers</a> model.</p> <p>This model was further improved by the <a href="https://arxiv.org/pdf/2210.09782.pdf" rel="external nofollow noopener" target="_blank">Decoupling Features in Hierarchical Propagation for Video Object Segmentation</a> (DeAOT) model. The key difference between the two being a Gated Propogation Module (GPM) that seperates object specific from object agnostic features. Within the GPM, attention is performed on local tokens and on a global tokens which uses historical information <a class="citation" href="#yang2022decoupling">(Yang &amp; Yang, 2022)</a>. The AOT model uses heiarchical propogation to trasfer information from past frames to current frame <a class="citation" href="#yang2021associating">(Yang et al., 2021)</a>. The DeAOT model achieves new SOTA on YouTube-VOS, DAVIS 2017, DAVIS 2016, and VOT 2020. On YouTube-VOS, DeAOT achieves 82% accuracy at 52FPS, meeting the project requirements.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/aot-480.webp 480w,/assets/img/aot-800.webp 800w,/assets/img/aot-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/aot.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deaot-480.webp 480w,/assets/img/deaot-800.webp 800w,/assets/img/deaot-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/deaot.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Visualization of DeAOT architecture </div> <p>The creators of the AOT model provide open source code. One benefit of their code base is each model is defined from in a highly cofigurable manner, making it extremly easy to change backbone, latent dimension, number of attention heads, type of attention, etc.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DefaultModelConfig</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="sh">'</span><span class="s">AOTDefault</span><span class="sh">'</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_VOS</span> <span class="o">=</span> <span class="sh">'</span><span class="s">aot</span><span class="sh">'</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_ENGINE</span> <span class="o">=</span> <span class="sh">'</span><span class="s">aotengine</span><span class="sh">'</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_ALIGN_CORNERS</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_ENCODER</span> <span class="o">=</span> <span class="sh">'</span><span class="s">mobilenetv2</span><span class="sh">'</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_ENCODER_PRETRAIN</span> <span class="o">=</span> <span class="sh">'</span><span class="s">./pretrain_models/mobilenet_v2-b0353104.pth</span><span class="sh">'</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_ENCODER_DIM</span> <span class="o">=</span> <span class="p">[</span><span class="mi">24</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">1280</span><span class="p">]</span>  <span class="c1"># 4x, 8x, 16x, 16x
</span>        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_ENCODER_EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">256</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_DECODER_INTERMEDIATE_LSTT</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_FREEZE_BN</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_FREEZE_BACKBONE</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_MAX_OBJ_NUM</span> <span class="o">=</span> <span class="mi">10</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_SELF_HEADS</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_ATT_HEADS</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_LSTT_NUM</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_EPSILON</span> <span class="o">=</span> <span class="mf">1e-5</span>
        <span class="n">self</span><span class="p">.</span><span class="n">MODEL_USE_PREV_PROB</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div> <h2 id="model-improvements">Model Improvements</h2> <p>One major limitation of the AOT code is that during model evaluation, the <code class="language-plaintext highlighter-rouge">Evaluator</code> object saves copies of the predicted annotation masks to disk. As we scale up the number of experiments, memory limitations become an issue. Instead of saving the masks, I refactored the <code class="language-plaintext highlighter-rouge">Evaluator</code> methods to calculated <code class="language-plaintext highlighter-rouge">IoU</code> and <code class="language-plaintext highlighter-rouge">FPS</code> on the fly. We also used distance from center of either prediction masks.</p> <p>Key evaluation improvements:</p> <ul> <li>IoU was previously computed by loading an predicted mask and truth mask from disk, then leveraging the <code class="language-plaintext highlighter-rouge">cv2</code> library. However, I already had pred and truth masks in tensor form, so I wrote a custom function to evaluate Iou from batched tensor form.</li> <li>When a predicted mask is blank, IoU would produce divison by zero error and distance from centroid produces a <code class="language-plaintext highlighter-rouge">nan</code>. I handled these edge cases by catching when <code class="language-plaintext highlighter-rouge">Union == 0</code> and replacing nans with <code class="language-plaintext highlighter-rouge">384</code> which is the furthest euclidian distance across a 256x256 image.</li> <li>Demo mode (which creates a video of )</li> <li>I tested the new evaluation changes by adding a <code class="language-plaintext highlighter-rouge">DISPLAY_MASKS</code> bit to the config which visualizes the pred, truth, and intersection in RGB respectively.</li> </ul> <p>Major challenge:</p> <blockquote> <p>The evaluation dataloader only provides one truth mask per <code class="language-plaintext highlighter-rouge">k</code> frames per sequence, because during inference you only need a single prompt for what you should be tracking (at <code class="language-plaintext highlighter-rouge">t=0</code>). The rest of the frames shouldn’t have truth information or else you wouldn’t need to predict. The <code class="language-plaintext highlighter-rouge">Evaluator</code> object takes advantage of every time a ground truth frame is passed by incorporating that information into assumptions about priors, adding considerable overhead to the <code class="language-plaintext highlighter-rouge">FPS</code>.</p> </blockquote> <p>DINO Backbone: The</p> <p>In progress:</p> <ul> <li>Add DINO backbone</li> <li>FlashAttention for more efficient GPU usage</li> <li>TokenMerging</li> <li>Add Layerwise learning rate decay for deepest models</li> <li>Sparser segmentation head</li> </ul> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="kirillov2023segment" class="col-sm-8"> <div class="title">Segment Anything</div> <div class="author"> Alexander Kirillov ,  Eric Mintun ,  Nikhila Ravi , and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="hu2022siammask" class="col-sm-8"> <div class="title">SiamMask: A Framework for Fast Online Object Tracking and Segmentation</div> <div class="author"> Weiming Hu ,  Qiang Wang ,  Li Zhang , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Luca Bertinetto, Philip H. S. Torr' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="yang2022decoupling" class="col-sm-8"> <div class="title">Decoupling Features in Hierarchical Propagation for Video Object Segmentation</div> <div class="author"> Zongxin Yang ,  and  Yi Yang </div> <div class="periodical"> 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="yang2021associating" class="col-sm-8"> <div class="title">Associating Objects with Transformers for Video Object Segmentation</div> <div class="author"> Zongxin Yang ,  Yunchao Wei ,  and  Yi Yang </div> <div class="periodical"> 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Tristan Peat. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>