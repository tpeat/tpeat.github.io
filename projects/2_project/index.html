<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Depth Estimation | Tristan Peat </title> <meta name="author" content="Tristan Peat"> <meta name="description" content="Simplified depth estimation transformer from monocular lens"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tpeat.github.io/projects/2_project/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tristan </span> Peat </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/tpeat_cv.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Depth Estimation</h1> <p class="post-description">Simplified depth estimation transformer from monocular lens</p> </header> <article> <hr> <h3 id="project-objective">Project Objective:</h3> <ul> <li>Design a depth estimation model that uses a single lens</li> <li>Collect data for long range depths in a variety of settings</li> <li>Evaluate the model for accuracy at depth ranges</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/monodepth-480.webp 480w,/assets/img/monodepth-800.webp 800w,/assets/img/monodepth-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/monodepth.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>I joined this project at Georgia Tech Research Institute (GTRI) after 2 semester from its start. While the model architecture and training pipeline was near complete, the team needed assistance in running experiments and evaluating the models.</p> <h3 id="key-contributions">Key Contributions</h3> <ul> <li>Developed a model evaluation script that bins pixelwise errors by depths</li> <li>Launched exeriments to evaluate various model configurations on a multi-GPU slurm cluster</li> </ul> <hr> <h3 id="data-sourcing">Data sourcing</h3> <p>The Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) dataset is one of the most famous datasets for a variety of vision and autonomous vehicle tasks. This dataset was used as a benchmark for the model on short range data, like that in a city landscape. This GIF shown above is an example of RGB and depth frames from a drive in an urban environment.</p> <p>To collect long range depth data, Microsoft’s AirSim simulator to generate depth data using cameras mounted on drones flying through a variety of scenes at a variety of altitudes. Clouds and wind were disabled from data collection. A combination of rural and city scenes were used for training.</p> <div class="row"> <div class="row-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/airsim-480.webp 480w,/assets/img/airsim-800.webp 800w,/assets/img/airsim-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/airsim.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/airsim2-480.webp 480w,/assets/img/airsim2-800.webp 800w,/assets/img/airsim2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/airsim2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Samples of depth data from Microsoft's Airsim </div> <h3 id="model-selection">Model Selection</h3> <p>Previous attempts at monocular lens depth estimation use some type of scaling to ground the depth predicitons to a ground truth value. That is, depth estimation models are good at predicting relative depths but without knowing how far away one element is, they are often several magnitudes away from truth values. To solve this in a self-supervised manner, without the need of passing any ground truth values to the depth net, the model architecture uses two sub networks during training <a class="citation" href="#monodepth2">(Godard et al., 2019)</a>.</p> <p>DepthNet to predict depths at a given frame and PerceiverNet to predict the pose between two frames. The pose net is only used during training to constrain the depth estimation network. Per pixel reprojection loss is used to fine tune the model.</p> <p>Depth Estimation with Simplified Transformers (DEST), uses stacked tranformer endoder blocks with residual connections to a pose net and to the decoder resampling blocks. <a class="citation" href="#yang2022depth">(Yang et al., 2022)</a>. Overlapped patch embedding is used to preserve local image context. Inspired by the feature pyramid network, DEST uses a progressively upsampling decoder using bilinear interpolation.</p> <div class="row"> <div class="row-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dest-480.webp 480w,/assets/img/dest-800.webp 800w,/assets/img/dest-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/dest.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Attention in transformer blocks are inherently quadratic time complexity when attending to all tokens in the image. Therefore, the DEST model focuses on simplifying the transformer design to achieve efficient inference. The attention mechanism applies a sequence reduction process to reduce K by reduction ratio \(R^2\). It also reaplced softmax with row-wise max pooling. Layer norms were also eliminated in favor of batch normalization which is precomputed from train statistics during inference.</p> <div class="row"> <div class="row-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dest-attention-480.webp 480w,/assets/img/dest-attention-800.webp 800w,/assets/img/dest-attention-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/dest-attention.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h3 id="model-improvements">Model Improvements</h3> <p>Inception style modules were used in replace of MixFFN and as a additional spatial reduction mechanism. Inception modules were used for increased parrellism and feature extraction at different granularities, <a class="citation" href="#szegedy2014going">(Szegedy et al., 2014)</a>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/inception-480.webp 480w,/assets/img/inception-800.webp 800w,/assets/img/inception-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/inception.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The inception block uses point wise convolutions to reduce the channelewise dimension and then max pooling and strided 3x3 convolutions to reduce the spatial reductions.</p> <h3 id="results">Results</h3> <p>Less than 5% MAE for up to 500m, less than 10% MAE for up to 1000m. This seriously expands the capability from other models trained on the KITTI dataset which rarely goes beyond 80m.</p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="yang2022depth" class="col-sm-8"> <div class="title">Depth Estimation with Simplified Transformer</div> <div class="author"> John Yang ,  Le An ,  Anurag Dixit , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Jinkyu Koo, Su Inn Park' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="monodepth2" class="col-sm-8"> <div class="title">Digging into Self-Supervised Monocular Depth Prediction</div> <div class="author"> Clément Godard ,  Oisin Mac Aodha ,  Michael Firman , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Gabriel J. Brostow' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> Oct 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="szegedy2014going" class="col-sm-8"> <div class="title">Going Deeper with Convolutions</div> <div class="author"> Christian Szegedy ,  Wei Liu ,  Yangqing Jia , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> Oct 2014 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Tristan Peat. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>