<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multimodal Foundation Models | Tristan Peat </title> <meta name="author" content="Tristan Peat"> <meta name="description" content="Reading list and notes on MLLM"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tpeat.github.io/blog/2024/mllm/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tristan </span> Peat </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/tpeat_cv.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Multimodal Foundation Models</h1> <p class="post-meta"> December 05, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/reading"> <i class="fa-solid fa-hashtag fa-sm"></i> reading</a>     ·   <a href="/blog/category/thoughts"> <i class="fa-solid fa-tag fa-sm"></i> thoughts</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="my-favorites">My Favorites</h1> <h3 id="chameleon-mixed-modal-early-fusion-foundation-models">Chameleon: Mixed-Modal Early-Fusion Foundation Models</h3> <p>The Chamelon work by Meta was one of my favorite reads in a while; I appreciate the focus on truly unifying modalities off the rip <a class="citation" href="#chameleonteam2024chameleonmixedmodalearlyfusionfoundation">(Team, 2024)</a>. It supports my vision that someday all modalities can just be plopped into a model and treated the same; basically abstract the model from knowing what modality it actually is. I think vision modalities still pose a problem to this because they are way higher dimension than text so they would unfairely hog cmopute unless we discrete them with a code book.</p> <p>The model’s architecture is particularly innovative for three main reasons: First, it uses a single transformer architecture that can handle both text and image data through a unified token system. This is analogous to speaking a single universal language rather than constantly translating between different languages. The model was trained on an impressive scale - approximately 10 trillion interleaved tokens - which helped it develop this unified understanding.</p> <p>Second, the researchers identified and solved a critical challenge in multimodal training: competition between different types of input. They discovered that different modalities would essentially “fight” for prominence during training, which they could observe through mathematical analysis of token norms. They developed solutions like QK normalization to prevent this competition, leading to more stable training.</p> <p>Third, the model demonstrates remarkable versatility. It can not only handle mixed image-text tasks as well as specialized models like Gemini and GPT-4V, but it also maintains strong performance on text-only tasks, outperforming LLaMA2 in most evaluations.</p> <p>I think it would be an interesting approach to weave the “thought tokens” from O1 models into the input stream and generate a mix of modality, thought, different modality tokens at the output. In integrate thoughts closer, to what we are actually producing, maybe we can induce better reasoning capabiltiies.</p> <h3 id="imagebind-one-embedding-space-to-bind-them-all">ImageBind: One Embedding Space To Bind Them All</h3> <p>Meta’s ImageBind creates a unified embedding space for six different modalities using only image-paired data as the connective tissue <a class="citation" href="#girdhar2023imagebindembeddingspacebind">(Girdhar et al., 2023)</a>. This innovative approach allows the model to bridge images, text, audio, depth, thermal, and IMU data in ways that enable remarkable zero-shot and few-shot learning capabilities across modalities with limited or no direct pairing data. By keeping pretrained image and text encoders frozen while fine-tuning encoders for other modalities, IMAGEBIND has demonstrated surprising “emergent alignment” properties, where modalities that seemingly shouldn’t be related show natural connections. While built upon CLIP’s architecture (and thus inheriting some of its limitations), IMAGEBIND has already surpassed supervised models like AudioMAE in zero-shot performance.</p> <p>I was quite suprised that images were the binding modality to unify the other 5. I actuall thought that language would be a better representation for the other modalities, as humans invented language to describe the physical world. I think passing modalities like EEG through a vision encoder makes no sense because we are assuming that the frozen vision modal has the capacity to represent the incoming modality as an image. This feels like a bottleneck to modalities that shouldn’t have an image representation.</p> <h3 id="learning-transferable-visual-models-from-natural-language-supervision">Learning Transferable Visual Models From Natural Language Supervision</h3> <p>An all time classic and what feels like a universal solution to open-vocabulary models: CLIP <a class="citation" href="#radford2021learningtransferablevisualmodels">(Radford et al., 2021)</a>. The CLIP model uses natural language supervision, (image, text) pairs scraped from the web to pre-train multiple models and classifiers to perform on a variety of benchmarks without optimizing specifically for a single benchmark. The main contribution of the paper was scaling the concepts of natural language for vision datasets and consequently created a powerful family of zero-shot models that are competitive with fully supervised models and robust to underlying distribution shifts.</p> <p>A really well written paper all around. The scaling analysis was thorough, evaluation on a billion metrics.</p> <p>Limitations: obviously there are concerns of data leakage between tasks.</p> <p>I think CLIPs inability to perform fine grained tasks is pretty interesting and begs the question at whether the captions just aren’t detailed enough, too noisy, or if the problem lies in the contrastive loss objective which might be optimizing for semantic concepts rather than background less-important concepts.</p> <h1 id="at-bat">At Bat</h1> <h3 id="agentinstruct-toward-generative-teaching-with-agentic-flows">AgentInstruct: Toward Generative Teaching with Agentic Flows</h3> <p>I believe the next big leap in AGI might come from training with videos and/or interactions with environments, the latter having the advantage that models can create their own data through the interactions <a class="citation" href="#mitra2024agentinstructgenerativeteachingagentic">(Mitra et al., 2024)</a>.</p> <p>There’s some research suggesting that scaling might be plateuring and thus companies like Anthropic or OpenAI are turning to agents; see OpenAI’s Operator.</p> <h3 id="vipact-visual-perception-enhancement-via-specialized-vlm-agent-collaboration-and-tool-use">VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use</h3> <p><a class="citation" href="#zhang2024vipactvisualperceptionenhancementspecialized">(Zhang et al., 2024)</a></p> <p>Reasoning tasks.</p> <h3 id="hugginggpt-solving-ai-tasks-with-chatgpt-and-its-friends-in-hugging-face">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</h3> <p><a class="citation" href="#shen2023hugginggptsolvingaitasks">(Shen et al., 2023)</a></p> <p>Reasoning tasks.</p> <h3 id="m3cot-a-novel-benchmark-for-multi-domain-multi-step-multi-modal-chain-of-thought">\(M^3\)CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought</h3> <p><a class="citation" href="#chen2024m3cotnovelbenchmarkmultidomain">(Chen et al., 2024)</a></p> <p>Techniques for multimodal ‘reasoning’ still lags behind reasoning in text-space by large margin, and seems a promise research avenue.</p> <p>There is a small gap between open source and commercial models such as GPT4 with these results</p> </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="chameleonteam2024chameleonmixedmodalearlyfusionfoundation" class="col-sm-8"> <div class="title">Chameleon: Mixed-Modal Early-Fusion Foundation Models</div> <div class="author"> Chameleon Team </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="mitra2024agentinstructgenerativeteachingagentic" class="col-sm-8"> <div class="title">AgentInstruct: Toward Generative Teaching with Agentic Flows</div> <div class="author"> Arindam Mitra ,  Luciano Del Corro ,  Guoqing Zheng , and <span class="more-authors" title="click to view 11 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '11 more authors' ? 'Shweti Mahajan, Dany Rouhana, Andres Codas, Yadong Lu, Wei-ge Chen, Olga Vrousgos, Corby Rosset, Fillipe Silva, Hamed Khanpour, Yash Lara, Ahmed Awadallah' : '11 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">11 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="zhang2024vipactvisualperceptionenhancementspecialized" class="col-sm-8"> <div class="title">VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use</div> <div class="author"> Zhehao Zhang ,  Ryan Rossi ,  Tong Yu , and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Franck Dernoncourt, Ruiyi Zhang, Jiuxiang Gu, Sungchul Kim, Xiang Chen, Zichao Wang, Nedim Lipka' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="chen2024m3cotnovelbenchmarkmultidomain" class="col-sm-8"> <div class="title">M^3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought</div> <div class="author"> Qiguang Chen ,  Libo Qin ,  Jin Zhang , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Zhi Chen, Xiao Xu, Wanxiang Che' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="girdhar2023imagebindembeddingspacebind" class="col-sm-8"> <div class="title">ImageBind: One Embedding Space To Bind Them All</div> <div class="author"> Rohit Girdhar ,  Alaaeldin El-Nouby ,  Zhuang Liu , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="shen2023hugginggptsolvingaitasks" class="col-sm-8"> <div class="title">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</div> <div class="author"> Yongliang Shen ,  Kaitao Song ,  Xu Tan , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Dongsheng Li, Weiming Lu, Yueting Zhuang' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="radford2021learningtransferablevisualmodels" class="col-sm-8"> <div class="title">Learning Transferable Visual Models From Natural Language Supervision</div> <div class="author"> Alec Radford ,  Jong Wook Kim ,  Chris Hallacy , and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Tristan Peat. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>