---
layout: post
title: Multimodal Foundation Models
date: 2024-12-05 15:09:00
description: Reading list and notes on MLLM
tags: reading
categories: thoughts
featured: false
related_publications: true
---

# My Favorites

### Chameleon: Mixed-Modal Early-Fusion Foundation Models

The Chamelon work by Meta was one of my favorite reads in a while; I appreciate the focus on truly unifying modalities off the rip {% cite chameleonteam2024chameleonmixedmodalearlyfusionfoundation %}. It supports my vision that someday all modalities can just be plopped into a model and treated the same; basically abstract the model from knowing what modality it actually is. I think vision modalities still pose a problem to this because they are way higher dimension than text so they would unfairely hog cmopute unless we discrete them with a code book.

The model's architecture is particularly innovative for three main reasons:
First, it uses a single transformer architecture that can handle both text and image data through a unified token system. This is analogous to speaking a single universal language rather than constantly translating between different languages. The model was trained on an impressive scale - approximately 10 trillion interleaved tokens - which helped it develop this unified understanding.

Second, the researchers identified and solved a critical challenge in multimodal training: competition between different types of input. They discovered that different modalities would essentially "fight" for prominence during training, which they could observe through mathematical analysis of token norms. They developed solutions like QK normalization to prevent this competition, leading to more stable training.

Third, the model demonstrates remarkable versatility. It can not only handle mixed image-text tasks as well as specialized models like Gemini and GPT-4V, but it also maintains strong performance on text-only tasks, outperforming LLaMA2 in most evaluations.

I think it would be an interesting approach to weave the "thought tokens" from O1 models into the input stream and generate a mix of modality, thought, different modality tokens at the output. In integrate thoughts closer, to what we are actually producing, maybe we can induce better reasoning capabiltiies. 


### ImageBind: One Embedding Space To Bind Them All

Meta's ImageBind creates a unified embedding space for six different modalities using only image-paired data as the connective tissue {% cite girdhar2023imagebindembeddingspacebind %}. This innovative approach allows the model to bridge images, text, audio, depth, thermal, and IMU data in ways that enable remarkable zero-shot and few-shot learning capabilities across modalities with limited or no direct pairing data. By keeping pretrained image and text encoders frozen while fine-tuning encoders for other modalities, IMAGEBIND has demonstrated surprising "emergent alignment" properties, where modalities that seemingly shouldn't be related show natural connections. While built upon CLIP's architecture (and thus inheriting some of its limitations), IMAGEBIND has already surpassed supervised models like AudioMAE in zero-shot performance. 

I was quite suprised that images were the binding modality to unify the other 5. I actuall thought that language would be a better representation for the other modalities, as humans invented language to describe the physical world. I think passing modalities like EEG through a vision encoder makes no sense because we are assuming that the frozen vision modal has the capacity to represent the incoming modality as an image. This feels like a bottleneck to modalities that shouldn't have an image representation. 

### Learning Transferable Visual Models From Natural Language Supervision

An all time classic and what feels like a universal solution to open-vocabulary models: CLIP {% cite radford2021learningtransferablevisualmodels %}. The CLIP model uses natural language supervision, (image, text) pairs scraped from the web to pre-train multiple models and classifiers to perform on a variety of benchmarks without optimizing specifically for a single benchmark. The main contribution of the paper was scaling the concepts of natural language for vision datasets and consequently created a powerful family of zero-shot models that are competitive with fully supervised models and robust to underlying distribution shifts. 

A really well written paper all around. The scaling analysis was thorough, evaluation on a billion metrics. 

Limitations: obviously there are concerns of data leakage between tasks. 

I think CLIPs inability to perform fine grained tasks is pretty interesting and begs the question at whether the captions just aren't detailed enough, too noisy, or if the problem lies in the contrastive loss objective which might be optimizing for semantic concepts rather than background less-important concepts. 


# At Bat

### AgentInstruct: Toward Generative Teaching with Agentic Flows

I believe the next big leap in AGI might come from training with videos and/or interactions with environments, the latter having the advantage that models can create their own data through the interactions {% cite mitra2024agentinstructgenerativeteachingagentic %}. 

There's some research suggesting that scaling might be plateuring and thus companies like Anthropic or OpenAI are turning to agents; see OpenAI's Operator. 

### VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use

{% cite zhang2024vipactvisualperceptionenhancementspecialized %}

Reasoning tasks.

### HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face

{% cite shen2023hugginggptsolvingaitasks %}

Reasoning tasks.


### $$M^3$$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought

{% cite chen2024m3cotnovelbenchmarkmultidomain %}

Techniques for multimodal 'reasoning' still lags behind reasoning in text-space by large margin, and seems a promise research avenue.

There is a  small gap between open source and commercial models such as GPT4 with these results
